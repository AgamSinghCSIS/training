{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75d776f1-c309-46bb-90c2-0e3c2864122b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "* Ensure access to stage table directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3d65a12-e370-4c5f-8e2a-469d5a109a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/stg.db/products_stg/_delta_log/</td><td>_delta_log/</td><td>0</td><td>1737593259000</td></tr><tr><td>dbfs:/user/hive/warehouse/stg.db/products_stg/part-00000-8d867354-0f64-4051-9300-61392d734354-c000.snappy.parquet</td><td>part-00000-8d867354-0f64-4051-9300-61392d734354-c000.snappy.parquet</td><td>2652</td><td>1737593262000</td></tr><tr><td>dbfs:/user/hive/warehouse/stg.db/products_stg/part-00001-41e0a0a0-4ef7-49f9-9ed9-bef55a28a9e2-c000.snappy.parquet</td><td>part-00001-41e0a0a0-4ef7-49f9-9ed9-bef55a28a9e2-c000.snappy.parquet</td><td>2670</td><td>1737593262000</td></tr><tr><td>dbfs:/user/hive/warehouse/stg.db/products_stg/part-00002-d6182139-8a64-4449-b1f0-a65cae042b1e-c000.snappy.parquet</td><td>part-00002-d6182139-8a64-4449-b1f0-a65cae042b1e-c000.snappy.parquet</td><td>2642</td><td>1737593262000</td></tr><tr><td>dbfs:/user/hive/warehouse/stg.db/products_stg/part-00003-d0ed4798-d1ef-460e-83e8-bea4080a610a-c000.snappy.parquet</td><td>part-00003-d0ed4798-d1ef-460e-83e8-bea4080a610a-c000.snappy.parquet</td><td>2485</td><td>1737593262000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/stg.db/products_stg/_delta_log/",
         "_delta_log/",
         0,
         1737593259000
        ],
        [
         "dbfs:/user/hive/warehouse/stg.db/products_stg/part-00000-8d867354-0f64-4051-9300-61392d734354-c000.snappy.parquet",
         "part-00000-8d867354-0f64-4051-9300-61392d734354-c000.snappy.parquet",
         2652,
         1737593262000
        ],
        [
         "dbfs:/user/hive/warehouse/stg.db/products_stg/part-00001-41e0a0a0-4ef7-49f9-9ed9-bef55a28a9e2-c000.snappy.parquet",
         "part-00001-41e0a0a0-4ef7-49f9-9ed9-bef55a28a9e2-c000.snappy.parquet",
         2670,
         1737593262000
        ],
        [
         "dbfs:/user/hive/warehouse/stg.db/products_stg/part-00002-d6182139-8a64-4449-b1f0-a65cae042b1e-c000.snappy.parquet",
         "part-00002-d6182139-8a64-4449-b1f0-a65cae042b1e-c000.snappy.parquet",
         2642,
         1737593262000
        ],
        [
         "dbfs:/user/hive/warehouse/stg.db/products_stg/part-00003-d0ed4798-d1ef-460e-83e8-bea4080a610a-c000.snappy.parquet",
         "part-00003-d0ed4798-d1ef-460e-83e8-bea4080a610a-c000.snappy.parquet",
         2485,
         1737593262000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls /user/hive/warehouse/stg.db/products_stg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0cee23a-4efd-46ed-ade2-b9f91a861765",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "(ONE TIME EXE FILE): Just to build Bz tables as expected \n",
    "1. Confirm the num of tables being put in bz layer are the same tables expected \n",
    "2. (ONCE) Get the schema from the each table and store it \n",
    "3. use this extracted schema to create table inside the bz-layer external location. \n",
    "Thats it. \n",
    "\n",
    "-- Below code only works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a384bfed-cd56-4711-b377-280dbcbcf872",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Class Automatically builds the Bronze layer, during the testing by using the schema of the staging layer \n",
    "# DOES NOT POPULATE Bronze layer tables, Just creates empty delta tables inside specified directories for holding delta table data \n",
    "# Use with the following variables to see execution \n",
    "# hive loc: '/user/hive/warehouse/'\n",
    "# stage_schema_name = 'stg'\n",
    "# bz_loc: 'abfss://bronze@datalakeselectivaproject.dfs.core.windows.net'\n",
    "# bz_schema_name = 'bz'\n",
    "\n",
    "class buildTableSchema():\n",
    "    def __init__(self, hive_loc, stage_schema_name, bz_loc, bz_schema_name):\n",
    "        self.stage_tables_base_loc = hive_loc\n",
    "        self.stage_schema_name     = stage_schema_name\n",
    "        self.stage_location        = self.stage_tables_base_loc + self.stage_schema_name + '.db'\n",
    "\n",
    "        self.bz_base_loc           = bz_loc\n",
    "        self.bz_schema_name        = bz_schema_name\n",
    "\n",
    "        # pre defined hardcoded table list\n",
    "        self.dim_table_list = ['channels_stg', 'costs_stg', 'customers_stg', 'products_stg', \n",
    "                               'promotions_stg', 'supplementary_demographics_stg', 'times_stg']\n",
    "        self.fact_table_list = ['sales_stg']\n",
    "\n",
    "    # Get list of tables in staging tables \n",
    "    # parameters : none\n",
    "    # returns    : List of table names in the order they appear in the stage layer    \n",
    "    def get_stage_table_list(self): \n",
    "        tb_list = []\n",
    "        for itr in dbutils.fs.ls(self.stage_location):\n",
    "            dir_name   = itr.name \n",
    "            table_name = dir_name.split('/')[0]\n",
    "            tb_list.append(table_name)\n",
    "        return tb_list\n",
    "    \n",
    "    # Compare the recieved list of tables in staging table dir with specified list of tables \n",
    "    # paramters : List of table names in staging directory \n",
    "    # returns   : Boolean if list matches -> true, else -> false \n",
    "    def check_stage_table_list(self, stg_table_list):\n",
    "        if (len(self.dim_table_list) + len(self.fact_table_list) == len(stg_table_list)) and (set(self.dim_table_list + self.fact_table_list) == set(stg_table_list)):\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Unexpected Number of stage tables: {len(stg_table_list)}\\nStage table list: {stg_table_list}\")\n",
    "            return False\n",
    "\n",
    "    # Main function to build the bronze layer tables\n",
    "    # parameters : list of table names in staging directory\n",
    "    # returns    : none \n",
    "    \"\"\" Description: \n",
    "        # For loop to iterate over each table in staging directory \n",
    "        # calculate the name for the resulting 'target table' \n",
    "        # get schema from 'staging table'\n",
    "        # create empty df using that extracted schema \n",
    "        # use the empty df write to create delta format data in 'target table *directory'\n",
    "        # use spark SQL to create a table inside the metastore catalog 'on-top' of that data \n",
    "        # check to make sure the schema of the empty df and table created have same num of columns \n",
    "    \"\"\"\n",
    "    def build_table(self, table_list):\n",
    "        for table in (table_list):\n",
    "            tgt_table = table[:len(table) - len(self.stage_schema_name) - 1] + '_' + self.bz_schema_name\n",
    "            print(f\"Processing table: {table}, target table: {tgt_table}\")\n",
    "\n",
    "            schema    = self.extract_schema(table)\n",
    "            e_df      = self.apply_schema_to_df(schema)\n",
    "            col_count = len(e_df.columns)\n",
    "            self.create_table_deltalog(tgt_table, e_df)\n",
    "            self.recreate_table_metadata(tgt_table, self.bz_schema_name)\n",
    "\n",
    "            desc_df   = spark.sql(f\"\"\"DESCRIBE `selectiva-project`.{self.bz_schema_name}.{tgt_table}\"\"\")\n",
    "            if (desc_df.count()) != (col_count + 3):\n",
    "                print(f\"Table {tgt_table} created BUT SOMETHING WENT WRONG WITH SCHEMA!!\")\n",
    "                print(f\"column count expected: {col_count} but got {desc_df.count()}\")\n",
    "                break\n",
    "            else:\n",
    "                print(f\"Table {tgt_table} created successfully!!\")\n",
    "                print('\\n')\n",
    "       \n",
    "    # Extract the schema from the staged table\n",
    "    # parameters : 'Stage table name' to extract the schema from \n",
    "    # returns    : Schema of the staged table \n",
    "    def extract_schema(self, table_name):\n",
    "        dlt_loc = self.stage_location + '/' + table_name\n",
    "        print(f\"\\tExtracting the schema from the '{table_name}' table\")\n",
    "        df      = (spark.read\n",
    "                    .format('delta')\n",
    "                    .option('header', 'true')\n",
    "                    .option('inferSchema','true')\n",
    "                    .load(dlt_loc))\n",
    "        return df.schema\n",
    "    \n",
    "    # Create an Empty Dataframe with a 'Fixed Schema'\n",
    "    # parameters : Schema to be applied to the Empty dataframe  \n",
    "    # returns    : Empty DF object with a fixed schema applied to it \n",
    "    def apply_schema_to_df(self, schema):\n",
    "        print(f\"\\tApplying schema to Empty dataframe and returning the dataframe\")\n",
    "        return (spark.createDataFrame([], schema))\n",
    "    \n",
    "    # Writes the Empty DF object to the 'bz layer' target-table location where data is to be kept \n",
    "    # variables  : Creates the target_table_dir variable as well as target_location variable \n",
    "    # parameters : 'Target-table-name', Empty DF object with fixed schema applied \n",
    "    # returns    : None, But creates a 'delta-log' dir inside the target-table-directory \n",
    "    def create_table_deltalog(self, table, df):\n",
    "        tgt_dir = table[:len(table) - len(self.bz_schema_name) -1] + '/'\n",
    "        loc     = self.bz_base_loc + '/' + tgt_dir \n",
    "        print(f\"\\tWriting Empty df with Extracted schema to {loc}\")\n",
    "        try:\n",
    "            df.write.format('delta').mode('overwrite').partitionBy(\"loaded_ts\").save(loc)\n",
    "        except:\n",
    "            print(f\"Error creating table {table} at {loc}. Don't add '/' at the end of contaier location\")\n",
    "\n",
    "    # Creates the External Tables on top of the data in the bronze layer cloud container in ADLS2 \n",
    "    # parameters : Target-table-name, Target-schema-name \n",
    "    # returns    : None, But creates the tables under the schema in project catalog \n",
    "    def recreate_table_metadata(self, target_table_name, target_schema_name):\n",
    "        target_dir = self.bz_base_loc + '/' + target_table_name[:len(target_table_name) - len(target_schema_name) - 1] + '/'\n",
    "        print(f\"\\tCreating the bronze table on top of delta file location in bronze layer\\n\\tTarget table:{target_table_name}\\n\\tTarget table dir:{target_dir}\")\n",
    "        #\"\"\"spark.sql(f\"\"\"DROP TABLE IF EXISTS `selectiva-project`.{target_schema_name}.{target_table_name}\"\"\")\"\"\"\n",
    "        spark.sql(f\"\"\"\n",
    "                  CREATE TABLE IF NOT EXISTS `selectiva-project`.{target_schema_name}.{target_table_name}\n",
    "                  USING DELTA \n",
    "                  LOCATION '{target_dir}'\n",
    "                    \"\"\")\n",
    "\n",
    "    # Function used for debugging purposes\n",
    "    def show(self):\n",
    "        print(f\"Passed parameters are: {self.stage_tables_base_loc}\\n{self.stage_schema_name}\\n{self.bz_base_loc}\\n{self.bz_schema_name}\")\n",
    "\n",
    "    # Driver Function for the class\n",
    "    def process(self):\n",
    "        print(f\"Creating Empty {self.bz_schema_name} tables with fixed Schemas\")\n",
    "        list_stg_tables = self.get_stage_table_list()  \n",
    "        if self.check_stage_table_list(list_stg_tables):\n",
    "            print(f\"Stage table number and names as expected!\")\n",
    "            self.build_table(list_stg_tables)   # Call to the main function to build the bronze layer tables\n",
    "        else:\n",
    "            print(f\"Something went wrong with the stage table list.\")\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a26175b-3524-4718-871a-12fdc3a6740d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Empty bz tables with fixed Schemas\nStage table number and names as expected!\nProcessing table: channels_stg, target table: channels_bz\n\tExtracting the schema from the 'channels_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/channels/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:channels_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/channels/\nTable channels_bz created successfully!!\n\n\nProcessing table: costs_stg, target table: costs_bz\n\tExtracting the schema from the 'costs_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/costs/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:costs_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/costs/\nTable costs_bz created successfully!!\n\n\nProcessing table: customers_stg, target table: customers_bz\n\tExtracting the schema from the 'customers_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/customers/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:customers_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/customers/\nTable customers_bz created successfully!!\n\n\nProcessing table: products_stg, target table: products_bz\n\tExtracting the schema from the 'products_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/products/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:products_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/products/\nTable products_bz created successfully!!\n\n\nProcessing table: promotions_stg, target table: promotions_bz\n\tExtracting the schema from the 'promotions_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/promotions/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:promotions_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/promotions/\nTable promotions_bz created successfully!!\n\n\nProcessing table: sales_stg, target table: sales_bz\n\tExtracting the schema from the 'sales_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/sales/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:sales_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/sales/\nTable sales_bz created successfully!!\n\n\nProcessing table: supplementary_demographics_stg, target table: supplementary_demographics_bz\n\tExtracting the schema from the 'supplementary_demographics_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/supplementary_demographics/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:supplementary_demographics_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/supplementary_demographics/\nTable supplementary_demographics_bz created successfully!!\n\n\nProcessing table: times_stg, target table: times_bz\n\tExtracting the schema from the 'times_stg' table\n\tApplying schema to Empty dataframe and returning the dataframe\n\tWriting Empty df with Extracted schema to abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/times/\n\tCreating the bronze table on top of delta file location in bronze layer\n\tTarget table:times_bz\n\tTarget table dir:abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/times/\nTable times_bz created successfully!!\n\n\n"
     ]
    }
   ],
   "source": [
    "bd = buildTableSchema('/user/hive/warehouse/','stg', 'abfss://bronze@datalakeselectivaproject.dfs.core.windows.net', 'bz')\n",
    "bd.process()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6186068542023378,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01-Schema-extraction-for-EXPECTED-tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}