{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6137932-78ae-468a-b717-ef90ec54c1d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# stageSchemaName    = 'stg'\n",
    "# stageTableBaseDir  = '/user/hive/warehouse/' + stageSchemaName + '.db/'\n",
    "# bronzeSchemaName   = 'bz'\n",
    "# bronzeTableBaseDir = 'abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/'\n",
    "\n",
    "class populateBronze():\n",
    "    def __init__(self, stageSchemaName, stageTableBaseDir, bronzeSchemaName, bronzeTableBaseDir):\n",
    "        self.stageSchemaName    = stageSchemaName\n",
    "        self.stageTableBaseDir  = stageTableBaseDir + stageSchemaName + '.db/'\n",
    "        self.bronzeSchemaName   = bronzeSchemaName\n",
    "        self.bronzeTableBaseDir = bronzeTableBaseDir\n",
    "\n",
    "    # get list the dirs in bz layer \n",
    "    def getBronzeTableList(self):\n",
    "        tableList = []\n",
    "        for table in dbutils.fs.ls(self.bronzeTableBaseDir):\n",
    "            tableList.append(table.name)\n",
    "        return tableList\n",
    "\n",
    "    def getStageTableList(self):\n",
    "        tableList = []\n",
    "        sub = len(self.stageSchemaName) + 2\n",
    "        for table in dbutils.fs.ls(self.stageTableBaseDir):\n",
    "            tableName = table.name[:len(table.name) - sub] + '/'\n",
    "            tableList.append(tableName)\n",
    "        return tableList\n",
    "\n",
    "    # using list, get data from stagetables to a df, return df \n",
    "    def getStageTableData(self, table):\n",
    "        stageTableName = table[:len(table) - 1] + '_' + self.stageSchemaName\n",
    "        sourceTablePath = self.stageTableBaseDir + stageTableName\n",
    "        print(f\"Loading Stage Table from: {sourceTablePath}\")\n",
    "        return (spark.read\n",
    "                     .format('delta')\n",
    "                     .option('header', 'true')\n",
    "                     .option('inferSchema', 'true')\n",
    "                     .load(sourceTablePath))\n",
    "        \n",
    "    # get df for bronze table from bz layer location \n",
    "    def getBronzeTableData(self, table):\n",
    "        bronzeTableName = table[:len(table) - 1] + '_' + self.bronzeSchemaName\n",
    "        targetTablePath = self.bronzeTableBaseDir + table\n",
    "        print(f\"Loading Bronze Table from: {targetTablePath}\")\n",
    "        return (DeltaTable.forPath(spark, targetTablePath))\n",
    "\n",
    "    # perform a delta lake merge to not ingest file with same name and loaded_ts again \n",
    "    def insertBronze(self, stageDf, bronzeDf):\n",
    "        (bronzeDf.alias('target')\n",
    "            .merge(stageDf.alias('source'), \"target.loaded_ts = source.loaded_ts \")\n",
    "            .whenNotMatchedInsertAll().execute()\n",
    "        )\n",
    "    # test the schema, (count num of columns or something )\n",
    "    def validateSchema(self, stageDf, bronzeDf):\n",
    "        return (stageDf.schema == bronzeDf.toDF().schema)\n",
    "    \n",
    "    # validate the tables, by counting number of rows \n",
    "    \"\"\"def validateTable(self, stageDf, bronzeDf):\n",
    "        return (stageDf.count() == bronzeDf.toDF().count())\"\"\"\n",
    "\n",
    "    def process(self):\n",
    "        bzTableList = self.getBronzeTableList()\n",
    "        stageTableList = self.getStageTableList()\n",
    "        processTables = [table for table in bzTableList if table in stageTableList]\n",
    "        for table in processTables:\n",
    "            print(f\"Processing table {table}\")\n",
    "            stage_df      = self.getStageTableData(table)\n",
    "            dlt_bronze_df = self.getBronzeTableData(table)\n",
    "            self.insertBronze(stage_df, dlt_bronze_df)\n",
    "            if (self.validateSchema(stage_df, dlt_bronze_df)):\n",
    "                print(f\"Table {table} populated successfully!\")\n",
    "            else:\n",
    "                print(f\"Table {table} schema validation failed! Please check.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f9e37eb-f4cb-4d59-863b-987de78f535b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table channels/\nLoading Stage Table from: /user/hive/warehouse/stg.db/channels_stg\nLoading Bronze Table from: abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/channels/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mDateTimeException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-5666400133891960>, line 2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m pB \u001B[38;5;241m=\u001B[39m populateBronze(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/user/hive/warehouse/\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbz\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mabfss://bronze@datalakeselectivaproject.dfs.core.windows.net/\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[0;32m----> 2\u001B[0m pB\u001B[38;5;241m.\u001B[39mprocess()\n",
       "\n",
       "File \u001B[0;32m<command-5666400133891959>, line 70\u001B[0m, in \u001B[0;36mpopulateBronze.process\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m     68\u001B[0m stage_df      \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetStageTableData(table)\n",
       "\u001B[1;32m     69\u001B[0m dlt_bronze_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetBronzeTableData(table)\n",
       "\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minsertBronze(stage_df, dlt_bronze_df)\n",
       "\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidateSchema(stage_df, dlt_bronze_df)):\n",
       "\u001B[1;32m     72\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTable \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m populated successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m<command-5666400133891959>, line 52\u001B[0m, in \u001B[0;36mpopulateBronze.insertBronze\u001B[0;34m(self, stageDf, bronzeDf)\u001B[0m\n",
       "\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minsertBronze\u001B[39m(\u001B[38;5;28mself\u001B[39m, stageDf, bronzeDf):\n",
       "\u001B[1;32m     50\u001B[0m     (bronzeDf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
       "\u001B[1;32m     51\u001B[0m         \u001B[38;5;241m.\u001B[39mmerge(stageDf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.loaded_ts = source.loaded_ts \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 52\u001B[0m         \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll()\u001B[38;5;241m.\u001B[39mexecute()\n",
       "\u001B[1;32m     53\u001B[0m     )\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/delta/tables.py:1230\u001B[0m, in \u001B[0;36mDeltaMergeBuilder.execute\u001B[0;34m(self)\u001B[0m\n",
       "\u001B[1;32m   1223\u001B[0m \u001B[38;5;129m@since\u001B[39m(\u001B[38;5;241m0.4\u001B[39m)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n",
       "\u001B[1;32m   1224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecute\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1225\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n",
       "\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    Execute the merge operation based on the built matched and not matched actions.\u001B[39;00m\n",
       "\u001B[1;32m   1227\u001B[0m \n",
       "\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001B[39;00m\n",
       "\u001B[1;32m   1229\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 1230\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jbuilder\u001B[38;5;241m.\u001B[39mexecute()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n",
       "\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mDateTimeException\u001B[0m: [CAST_INVALID_INPUT] The value 'abcd' of the type \"STRING\" cannot be cast to \"TIMESTAMP\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "DateTimeException",
        "evalue": "[CAST_INVALID_INPUT] The value 'abcd' of the type \"STRING\" cannot be cast to \"TIMESTAMP\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
       },
       "metadata": {
        "errorSummary": "[CAST_INVALID_INPUT] The value 'abcd' of the type \"STRING\" cannot be cast to \"TIMESTAMP\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "CAST_INVALID_INPUT",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "22018",
        "stackTrace": null,
        "startIndex": -1,
        "stopIndex": -1
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mDateTimeException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-5666400133891960>, line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m pB \u001B[38;5;241m=\u001B[39m populateBronze(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstg\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/user/hive/warehouse/\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbz\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mabfss://bronze@datalakeselectivaproject.dfs.core.windows.net/\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m pB\u001B[38;5;241m.\u001B[39mprocess()\n",
        "File \u001B[0;32m<command-5666400133891959>, line 70\u001B[0m, in \u001B[0;36mpopulateBronze.process\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     68\u001B[0m stage_df      \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetStageTableData(table)\n\u001B[1;32m     69\u001B[0m dlt_bronze_df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgetBronzeTableData(table)\n\u001B[0;32m---> 70\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minsertBronze(stage_df, dlt_bronze_df)\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalidateSchema(stage_df, dlt_bronze_df)):\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTable \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtable\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m populated successfully!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
        "File \u001B[0;32m<command-5666400133891959>, line 52\u001B[0m, in \u001B[0;36mpopulateBronze.insertBronze\u001B[0;34m(self, stageDf, bronzeDf)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minsertBronze\u001B[39m(\u001B[38;5;28mself\u001B[39m, stageDf, bronzeDf):\n\u001B[1;32m     50\u001B[0m     (bronzeDf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     51\u001B[0m         \u001B[38;5;241m.\u001B[39mmerge(stageDf\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msource\u001B[39m\u001B[38;5;124m'\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtarget.loaded_ts = source.loaded_ts \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 52\u001B[0m         \u001B[38;5;241m.\u001B[39mwhenNotMatchedInsertAll()\u001B[38;5;241m.\u001B[39mexecute()\n\u001B[1;32m     53\u001B[0m     )\n",
        "File \u001B[0;32m/databricks/spark/python/delta/tables.py:1230\u001B[0m, in \u001B[0;36mDeltaMergeBuilder.execute\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1223\u001B[0m \u001B[38;5;129m@since\u001B[39m(\u001B[38;5;241m0.4\u001B[39m)  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m   1224\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecute\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1225\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1226\u001B[0m \u001B[38;5;124;03m    Execute the merge operation based on the built matched and not matched actions.\u001B[39;00m\n\u001B[1;32m   1227\u001B[0m \n\u001B[1;32m   1228\u001B[0m \u001B[38;5;124;03m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001B[39;00m\n\u001B[1;32m   1229\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1230\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jbuilder\u001B[38;5;241m.\u001B[39mexecute()\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1356\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:261\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    257\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mDateTimeException\u001B[0m: [CAST_INVALID_INPUT] The value 'abcd' of the type \"STRING\" cannot be cast to \"TIMESTAMP\" because it is malformed. Correct the value as per the syntax, or change its target type. Use `try_cast` to tolerate malformed input and return NULL instead. If necessary set \"spark.sql.ansi.enabled\" to \"false\" to bypass this error. SQLSTATE: 22018"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pB = populateBronze('stg', '/user/hive/warehouse/', 'bz', 'abfss://bronze@datalakeselectivaproject.dfs.core.windows.net/')\n",
    "pB.process()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6186068542023374,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Populate Bronze layer tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}